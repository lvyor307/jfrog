{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuTSqPbf5uQt"
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"https://speedmedia.jfrog.com/08612fe1-9391-4cf3-ac1a-6dd49c36b276/https://media.jfrog.com/wp-content/uploads/2021/12/29113553/jfrog-logo-2022.svg/w_1024\" width=\"150\" height=\"150\">\n",
    "</div>\n",
    "\n",
    "# JFrog Data Science Home Assignment\n",
    "\n",
    "\n",
    "You are employed at a company, FrogAd Corp, that provides a platform for managing advertising campaigns on behalf of various businesses.\n",
    "\n",
    "FrogAd offers three distinct product tiers: P1, P2, and P3, with P3 being the highest-tier offering, followed by P2, and then P1. We've gathered data from customers who were in either the P1 or P2 tiers and examined whether they upgraded to the P3 tier or not. Our goal is to assist the Sales department in determining which customers should be their primary focus for upgrading to the P3 tier.\n",
    "\n",
    "Your assignment involves the following objectives:\n",
    "\n",
    "1. Conduct an analysis of customer data pertaining to P1 and P2 tiers.\n",
    "2. Develop a baseline model capable of predicting which customers are inclined to upgrade their subscription to the more advanced P3 tier. Discuss which metric(s) is/are relevant for this case.\n",
    "3. Suggest action items based on your findings. E.g., for your team or for business stakeholders.\n",
    "4. Prepare a summary presentation showing your development to the FrogAd staff. You will present this presentation to JFrog's team and answer questions regarding it. Please submit this presnetation file as well as the Jupyter notebook ipynb file you used. \n",
    "\n",
    "The data available contains:\n",
    "- The impressions from 4 different social media in the past 3 months\n",
    "- The client's industry type\n",
    "- The number of support cases were opened with us by the client\n",
    "- The number of technical sessions were held by us with the client\n",
    "- The number of platform users we had in the past 3 months\n",
    "- The number of people working for the customer whose contact information we have\n",
    "- The client's location\n",
    "- The client's subscription tier\n",
    "- The client's number of employees\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install & Import Required Libraries"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! pip install jupyter\n",
    "! pip install pandas\n",
    "! pip install pyarrow\n",
    "! pip install matplotlib\n",
    "! pip install scipy\n",
    "! pip install scikit-learn\n",
    "! pip install scikit-optimize\n",
    "! pip install lightgbm\n",
    "! pip install imbalanced-learn\n",
    "! pip install shap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chisquare\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from lightgbm import LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Exploration and EDA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "df = pd.read_parquet('assignment_data_(3).parquet')\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nans analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if there are missing values (NA) in accounts that are marked as sharing data (is_sharing = 1)\n",
    "df[df['is_sharing'] == 1].isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example data\n",
    "data = {'is_sharing': [0, 1], 'missing_values': [14782, 59]}\n",
    "missing_df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(missing_df['is_sharing'], missing_df['missing_values'], color=['orange', 'skyblue'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Total Missing Values by is_sharing')\n",
    "plt.xlabel('is_sharing (0 = Not Sharing, 1 = Sharing)')\n",
    "plt.ylabel('Count of Missing Values')\n",
    "plt.xticks([0, 1], labels=['Not Sharing', 'Sharing'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Annotate values on the bars\n",
    "for i, v in enumerate(missing_df['missing_values']):\n",
    "    ax.text(i, v + 100, str(v), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tiers Distribution"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "distribution = df.groupby(['product', 'target']).size().unstack()\n",
    "percentage_distribution = distribution.div(distribution.sum(axis=1), axis=0) * 100\n",
    "print(percentage_distribution)\n",
    "# Data\n",
    "upgrade_rates = percentage_distribution.T.to_dict()\n",
    "plot_data = pd.DataFrame(upgrade_rates)\n",
    "plot_data['index'] = ['Not Upgraded', 'Upgraded']\n",
    "plot_data = plot_data.set_index('index')\n",
    "\n",
    "# Stacked bar chart\n",
    "ax = plot_data.T.plot(kind='bar', stacked=True, figsize=(8, 5), color=['skyblue', 'orange'])\n",
    "plt.title('Upgrade Distribution by Product Tier')\n",
    "plt.xlabel('Product Tier')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Upgrade Status')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "contingency_table = pd.DataFrame({\n",
    "    'Not Upgraded (0)': [2616 * 0.88, 886 * 0.72],\n",
    "    'Upgraded (1)': [886 * 0.12, 351 * 0.28]\n",
    "}, index=['P1', 'P2'])\n",
    "\n",
    "# Chi-Square\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square: {chi2}, p-value: {p}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df['date'].min())\n",
    "print(df['date'].max())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regression and correlation analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_scatter_with_regression(x, y, title, color, ax):\n",
    "    # Scatter plot\n",
    "    ax.scatter(x, y, color=color, alpha=0.6, label=f'{x.name} vs {y.name}')\n",
    "    \n",
    "    # Fit a linear regression model\n",
    "    model = LinearRegression()\n",
    "    x_reshaped = x.values.reshape(-1, 1)\n",
    "    model.fit(x_reshaped, y)\n",
    "    y_pred = model.predict(x_reshaped)\n",
    "    \n",
    "    # Plot the regression line\n",
    "    ax.plot(x, y_pred, color=color, label='Regression Line')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(x.name, fontsize=12)\n",
    "    ax.set_ylabel(y.name, fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Plot settings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "no_nans_df_for_lr = df.dropna()\n",
    "\n",
    "# Plot for facebook vs x\n",
    "plot_scatter_with_regression(\n",
    "    no_nans_df_for_lr['facebook_impressions_1_lag'], \n",
    "    no_nans_df_for_lr['x_impressions_1_lag'], \n",
    "    \"Facebook vs X\", \n",
    "    'red', \n",
    "    axes[0]\n",
    ")\n",
    "\n",
    "# Plot for facebook vs instagram\n",
    "plot_scatter_with_regression(\n",
    "    no_nans_df_for_lr['facebook_impressions_1_lag'], \n",
    "    no_nans_df_for_lr['instagram_impressions_1_lag'], \n",
    "    \"Facebook vs Instagram\", \n",
    "    'blue', \n",
    "    axes[1]\n",
    ")\n",
    "\n",
    "# Plot for x vs instagram\n",
    "plot_scatter_with_regression(\n",
    "    no_nans_df_for_lr['x_impressions_1_lag'], \n",
    "    no_nans_df_for_lr['instagram_impressions_1_lag'], \n",
    "    \"X vs Instagram\", \n",
    "    'green', \n",
    "    axes[2]\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Box-Plots"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Melt the data for easy plotting\n",
    "melted_df = pd.melt(df, id_vars=\"target\", value_vars=[\"x_impressions_1_lag\", \"instagram_impressions_1_lag\", \"facebook_impressions_1_lag\"],\n",
    "                    var_name=\"Network\", value_name=\"Usage\")\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "boxplot = sns.boxplot(x=\"Network\", y=\"Usage\", hue=\"target\", data=melted_df)\n",
    "\n",
    "# Add plot titles and labels\n",
    "plt.title(\"Usage Distribution by Network with Target Separation\", fontsize=14)\n",
    "plt.xlabel(\"Network\", fontsize=12)\n",
    "plt.ylabel(\"Usage\", fontsize=12)\n",
    "plt.legend(title=\"Target\", loc=\"upper right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply a log transformation to the Usage column\n",
    "melted_df['Usage'] = np.log1p(melted_df['Usage'])  # log1p handles log(0) safely\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "boxplot = sns.boxplot(x=\"Network\", y=\"Usage\", hue=\"target\", data=melted_df)\n",
    "\n",
    "# Add plot titles and labels\n",
    "plt.title(\"Log-Transformed Usage Distribution by Network with Target Separation\", fontsize=14)\n",
    "plt.xlabel(\"Network\", fontsize=12)\n",
    "plt.ylabel(\"Log(Usage)\", fontsize=12)\n",
    "plt.legend(title=\"Target\", loc=\"upper right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Categorical Analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = [col for col in df if col not in numerical_features]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the categorical features and the target\n",
    "target = 'target'\n",
    "\n",
    "# Iterate through each categorical feature and plot the relationship with the target\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=feature, hue=target, palette=\"Set2\")\n",
    "    plt.title(f'Relationship Between {feature} and Target', fontsize=14)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.legend(title='Target')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_features = ['industry_group', 'territory', 'product', 'n_employees_range']\n",
    "target = 'target'\n",
    "\n",
    "# Overall target distribution\n",
    "overall_target_distribution = df[target].value_counts(normalize=True)\n",
    "\n",
    "# Function to perform the Chi-Square Test for each category\n",
    "def chi_square_per_category(df, feature, target, overall_dist):\n",
    "    results = []\n",
    "    for category in df[feature].unique():\n",
    "        # Subset target distribution for the current category\n",
    "        category_distribution = df[df[feature] == category][target].value_counts(normalize=True)\n",
    "        \n",
    "        # Align the distributions to ensure matching categories\n",
    "        category_dist = category_distribution.reindex(overall_dist.index, fill_value=0)\n",
    "        overall_dist_for_test = overall_dist.reindex(category_dist.index, fill_value=0)\n",
    "        \n",
    "        # Perform Chi-Square Test\n",
    "        chi2_stat, p_value = chisquare(f_obs=category_dist * len(df[df[feature] == category]),\n",
    "                                       f_exp=overall_dist_for_test * len(df[df[feature] == category]))\n",
    "        \n",
    "        results.append({\n",
    "            \"Feature\": feature,\n",
    "            \"Category\": category,\n",
    "            \"Chi2 Statistic\": chi2_stat,\n",
    "            \"P-value\": p_value,\n",
    "            \"Significant\": p_value < 0.05  # True if p-value is less than 0.05\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Perform the test for all categorical features\n",
    "all_results = []\n",
    "for feature in categorical_features:\n",
    "    results = chi_square_per_category(df, feature, target, overall_target_distribution)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Convert results to a DataFrame for easier visualization\n",
    "chi_square_results = pd.DataFrame(all_results)\n",
    "chi_square_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter significant categories\n",
    "significant_results = chi_square_results[chi_square_results['Significant']]\n",
    "\n",
    "# Prepare data for all significant categories\n",
    "plot_data = []\n",
    "\n",
    "for _, row in significant_results.iterrows():\n",
    "    feature = row['Feature']\n",
    "    category = row['Category']\n",
    "    \n",
    "    # Filter the DataFrame for the current category\n",
    "    subset = df[df[feature] == category]\n",
    "    \n",
    "    # Calculate target distribution in percentages\n",
    "    target_distribution = subset[target].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Add data for plotting\n",
    "    plot_data.append({\n",
    "        \"Feature\": feature,\n",
    "        \"Category\": category,\n",
    "        \"Target 0 (%)\": target_distribution.get(0, 0),\n",
    "        \"Target 1 (%)\": target_distribution.get(1, 0)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Combine feature and category for x-axis labels\n",
    "plot_df[\"Feature_Category\"] = plot_df[\"Feature\"] + \" (\" + plot_df[\"Category\"].astype(str) + \")\"\n",
    "\n",
    "# Add overall target distribution\n",
    "overall_target_distribution = df[target].value_counts(normalize=True) * 100\n",
    "overall_target_df = pd.DataFrame({\n",
    "    \"Feature_Category\": [\"Overall Target\"],\n",
    "    \"Target 0 (%)\": [overall_target_distribution.get(0, 0)],\n",
    "    \"Target 1 (%)\": [overall_target_distribution.get(1, 0)]\n",
    "})\n",
    "\n",
    "# Combine the significant categories with the overall target\n",
    "plot_df = pd.concat([plot_df, overall_target_df], ignore_index=True)\n",
    "\n",
    "# Plot\n",
    "ax = plot_df.set_index(\"Feature_Category\")[[\"Target 0 (%)\", \"Target 1 (%)\"]].plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(12, 6),\n",
    "    color=[\"skyblue\", \"salmon\"],\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Customize overall target bar color\n",
    "bars = ax.patches[-2:]  # The last two bars correspond to Overall Target (Target 0 and Target 1)\n",
    "bars[0].set_color(\"gray\")    # Target 0 for Overall Target\n",
    "bars[1].set_color(\"darkred\") # Target 1 for Overall Target\n",
    "bars[0].set_alpha(0.9)\n",
    "bars[1].set_alpha(0.9)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Target Distribution for Significant Categories and Overall Target\", fontsize=14)\n",
    "plt.xlabel(\"Feature (Category)\", fontsize=12)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Move legend to the center below the plot\n",
    "plt.legend(title=\"Target Value\", loc='lower center', bbox_to_anchor=(0.5, 0.3), ncol=2)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modeling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "df = df.set_index(['date', 'account_id'], drop=True)\n",
    "features = df.drop(columns=['target'])\n",
    "target = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42, stratify=target\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_features = features.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = [col for col in features if col not in numerical_features]\n",
    "\n",
    "constant_feature_droper = VarianceThreshold(threshold=0.05)\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),  # Standard scaling for numerical features\n",
    "    (\"imputer\", KNNImputer()),   # KNN Imputer for numerical data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute with most frequent value\n",
    "    (\"encoder\", OneHotEncoder())             # OneHotEncoder encoding for categorical features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"drop_constants\", constant_feature_droper),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_classif)),  # Feature selection\n",
    "    (\"classifier\", LGBMClassifier(random_state=42, class_weight=\"balanced\"))  # Classifier\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid for Bayesian search\n",
    "param_grid = {\n",
    "    # Tune threshold for constant/low-variance features\n",
    "    \"drop_constants__threshold\": (0.0, 0.01),\n",
    "    # Tune k for the KNN Imputer\n",
    "    \"preprocessor__num__imputer__n_neighbors\": (2, 10),  # Number of neighbors for numerical imputer\n",
    "    # Feature selection\n",
    "    \"feature_selection__k\": (6, len(numerical_features) + len(categorical_features)),  # Number of features to select\n",
    "    # LGBM hyperparameters\n",
    "    \"classifier__n_estimators\": (50, 500),         # Number of boosting rounds (trees)\n",
    "    \"classifier__max_depth\": (1, 10),              # Maximum depth of a tree\n",
    "    \"classifier__learning_rate\": (0.01, 0.3),      # Learning rate (shrinkage)\n",
    "    \"classifier__subsample\": (0.5, 1.0),           # Subsample ratio of training instances\n",
    "    \"classifier__colsample_bytree\": (0.5, 1.0),    # Subsample ratio of columns for each tree\n",
    "    \"classifier__min_child_samples\": (5, 30),      # Tune lower values\n",
    "    \"classifier__min_child_weight\": (1, 5),        # Allow lighter leaves\n",
    "    \"classifier__scale_pos_weight\": (1, 10)        # Balancing class weights\n",
    "}\n",
    "\n",
    "scoring_metric = make_scorer(fbeta_score, beta=0.5, average=\"binary\")\n",
    "# Define Bayesian search\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=pipeline,\n",
    "    search_spaces=param_grid,\n",
    "    n_iter=50,  # Number of iterations for the search\n",
    "    cv=5,       # Cross-validation splits\n",
    "    scoring=scoring_metric,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best CV Score:\", bayes_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_score = bayes_search.score(X_test, y_test)\n",
    "print(\"Test Score:\", test_score)\n",
    "y_pred = bayes_search.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(classification_report(y_test, y_pred))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shap Analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# EXtract the tuned classifier\n",
    "best_pipeline = bayes_search.best_estimator_\n",
    "classifier = best_pipeline.named_steps['classifier']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare shap explainer\n",
    "# Prepare the data (use the preprocessing step from the pipeline)\n",
    "preprocessed_X_train = best_pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(classifier, preprocessed_X_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get Features Names for shap analysis\n",
    "# Get the preprocessor step from the pipeline\n",
    "preprocessor = best_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Extract feature names for numerical and categorical features\n",
    "numerical_feature_names = numerical_features  # These remain the same\n",
    "categorical_feature_names = (\n",
    "    preprocessor.transformers_[1][1]\n",
    "    .named_steps['encoder']\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "# Combine numerical and categorical feature names\n",
    "feature_names = list(numerical_feature_names) + list(categorical_feature_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute SHAP values\n",
    "preprocessed_X_test = best_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "shap_values = explainer(preprocessed_X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assign feature names to SHAP values (if necessary)\n",
    "shap_values.feature_names = feature_names\n",
    "\n",
    "# Plot SHAP bar chart\n",
    "shap.plots.bar(shap_values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
